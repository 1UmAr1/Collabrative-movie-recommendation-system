# Removing INcomplete cases
incomplete.cases <- which(!complete.cases(AC.tfidf))
AC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(AC.tfidf))
incomplete.cases <- which(!complete.cases(AC.tfidf))
AC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(AC.tfidf))
library(irlba)
gc()
registerDoSNOW(cl)
AC.irlba.1 <- irlba(t(AC.tfidf), nv = 10, maxit = 600)
AC.svd.1 <- data.frame(AC.irlba.1$v)
################ Production Companies
# Production Companies
Data3 <- paste(Data$PDC1,"+", Data$PDC2)
Data3 <- as.data.frame(Data3)
# Tokenizing
# Tokenizing
PDC_tokens <- tokens(Data3$Data3, what="word", remove_symbols = T, remove_numbers = T, remove_url = T)
# Lower Case
PDC_tokens <- tokens_tolower(PDC_tokens)
PDC_tokens <- tokens_select(PDC_tokens, stopwords(), selection = "remove")
# Stemming
PDC_tokens <- tokens_wordstem(PDC_tokens, language = "english")
# biigrams
PDC_tokens <- tokens_ngrams(PDC_tokens, n = 2)
# Document Feature Matrix
PDC_dfm <- dfm(PDC_tokens, tolower = F)
# Matrix
PDC_matrix <- as.matrix(PDC_dfm)
library(doSNOW)
gc()
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
# TF
PDC.df <- apply(PDC_matrix, 1, term.frequency)
# IDf
PDC.idf <-apply(PDC_matrix, 2, inverse.doc.freq)
# TF_IDF
PDC.tfidf <- apply(PDC.df, 2, tf.idf,
idf = PDC.idf)
PDC.tfidf <- t(PDC.tfidf)
# Removing INcomplete cases
incomplete.cases <- which(!complete.cases(PDC.tfidf))
PDC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(PDC.tfidf))
incomplete.cases <- which(!complete.cases(PDC.tfidf))
PDC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(PDC.tfidf))
library(irlba)
gc()
registerDoSNOW(cl)
PDC.irlba.1 <- irlba(t(PDC.tfidf), nv = 10, maxit = 600)
PDC.svd.1 <- data.frame(PDC.irlba.1$v)
Data <- read.csv("Precossed_Data.csv", header = T)
credits <- read.csv(file = "Data/tmdb_5000_credits.csv", header= T)
######################Genre
Data1 <- paste(Data$genre_1, Data$genre_2, Data$genre_3)
Data1 <- as.data.frame(Data1)
# Tokenizing
genre_tokens <- tokens(Data1$Data1, what="word", remove_symbols = T, remove_numbers = T, remove_url = T)
# Lower Case
genre_tokens <- tokens_tolower(genre_tokens)
genre_tokens <- tokens_select(genre_tokens, stopwords(), selection = "remove")
# Stemming
genre_tokens <- tokens_wordstem(genre_tokens, language = "english")
# Unigrams
genre_tokens <- tokens_ngrams(genre_tokens, n = 1)
# Document Feature Matrix
genre_dfm <- dfm(genre_tokens, tolower = F)
# Matrix
genre_matrix <- as.matrix(genre_dfm)
#########TF IDF
term.frequency <- function(row){
row / sum(row)
}
# Take the row and divide by the total of the row
# Function for calculating inverse document frequency
# We want to look at the columns . so col
inverse.doc.freq <- function(col){
# Calculating documents in the columns
corpus.size <- length(col)
# Getting the number of rows where the column in not zero
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
# Function for calcularing TF-IDF
tf.idf <- function(x, idf){
x * idf
}
library(doSNOW)
gc()
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
# TF
genre.df <- apply(genre_matrix, 1, term.frequency)
# IDf
genre.idf <-apply(genre_matrix, 2, inverse.doc.freq)
# TF_IDF
genre.tfidf <- apply(genre.df, 2, tf.idf,
idf = genre.idf)
genre.tfidf <- t(genre.tfidf)
# Removing INcomplete cases
incomplete.cases <- which(!complete.cases(genre.tfidf))
genre.tfidf[incomplete.cases, ] <- rep(0.0, ncol(genre.tfidf))
incomplete.cases <- which(!complete.cases(genre.tfidf))
genre.tfidf[incomplete.cases, ] <- rep(0.0, ncol(genre.tfidf))
# Method:
# Dimensionality reduction using SVD
# Dimensionality Reduction
library(irlba)
gc()
registerDoSNOW(cl)
genre.irlba.1 <- irlba(t(genre.tfidf), nv = 10, maxit = 600)
genre.svd.1 <- data.frame(genre.irlba.1$v)
# ACTORS
Data2 <- paste(Data$actor_1,"+", Data$actor_2, "+",Data$actor_3)
Data2 <- as.data.frame(Data2)
# Tokenizing
# Tokenizing
AC_tokens <- tokens(Data2$Data2, what="word", remove_symbols = T, remove_numbers = T, remove_url = T)
# Lower Case
AC_tokens <- tokens_tolower(AC_tokens)
AC_tokens <- tokens_select(AC_tokens, stopwords(), selection = "remove")
# Stemming
AC_tokens <- tokens_wordstem(AC_tokens, language = "english")
# biigrams
AC_tokens <- tokens_ngrams(AC_tokens, n = 2)
# Document Feature Matrix
AC_dfm <- dfm(AC_tokens, tolower = F)
# Matrix
AC_matrix <- as.matrix(AC_dfm)
library(doSNOW)
gc()
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
# TF
AC.df <- apply(AC_matrix, 1, term.frequency)
# IDf
AC.idf <-apply(AC_matrix, 2, inverse.doc.freq)
# TF_IDF
AC.tfidf <- apply(AC.df, 2, tf.idf,
idf = AC.idf)
AC.tfidf <- t(AC.tfidf)
# Removing INcomplete cases
incomplete.cases <- which(!complete.cases(AC.tfidf))
AC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(AC.tfidf))
incomplete.cases <- which(!complete.cases(AC.tfidf))
AC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(AC.tfidf))
library(irlba)
gc()
registerDoSNOW(cl)
AC.irlba.1 <- irlba(t(AC.tfidf), nv = 10, maxit = 600)
AC.svd.1 <- data.frame(AC.irlba.1$v)
################ Production Companies
# Production Companies
Data3 <- paste(Data$PDC1,"+", Data$PDC2)
Data3 <- as.data.frame(Data3)
# Tokenizing
# Tokenizing
PDC_tokens <- tokens(Data3$Data3, what="word", remove_symbols = T, remove_numbers = T, remove_url = T)
# Lower Case
PDC_tokens <- tokens_tolower(PDC_tokens)
PDC_tokens <- tokens_select(PDC_tokens, stopwords(), selection = "remove")
# Stemming
PDC_tokens <- tokens_wordstem(PDC_tokens, language = "english")
# biigrams
PDC_tokens <- tokens_ngrams(PDC_tokens, n = 2)
# Document Feature Matrix
PDC_dfm <- dfm(PDC_tokens, tolower = F)
# Matrix
PDC_matrix <- as.matrix(PDC_dfm)
library(doSNOW)
gc()
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
# TF
PDC.df <- apply(PDC_matrix, 1, term.frequency)
# IDf
PDC.idf <-apply(PDC_matrix, 2, inverse.doc.freq)
# TF_IDF
PDC.tfidf <- apply(PDC.df, 2, tf.idf,
idf = PDC.idf)
PDC.tfidf <- t(PDC.tfidf)
# Removing INcomplete cases
incomplete.cases <- which(!complete.cases(PDC.tfidf))
PDC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(PDC.tfidf))
incomplete.cases <- which(!complete.cases(PDC.tfidf))
PDC.tfidf[incomplete.cases, ] <- rep(0.0, ncol(PDC.tfidf))
library(irlba)
gc()
registerDoSNOW(cl)
PDC.irlba.1 <- irlba(t(PDC.tfidf), nv = 10, maxit = 600)
PDC.svd.1 <- data.frame(PDC.irlba.1$v)
Data4 <- cbind(Data, genre.svd.1)
Data5 <- cbind(Data4, PDC.svd.1)
Data6 <- cbind(Data5, AC.svd.1)
View(Data6)
# Triming Down the feature we don't need
Data6 <- Data6[, -c(30:129)]
View(Data6)
2, 3, 5, 7, 8, 10, 11, 15, 17, 21, 22, 23, 25, 26, 2728, 29
Data6 <- Data6[,-c( 2, 3, 5, 7, 8, 10, 11, 15, 17, 21, 22, 23, 25, 26, 27, 28, 29)]
View(Data6)
DR_tokens <- tokens(Data$director, what="word", remove_symbols = T, remove_numbers = T, remove_url = T)
# Lower Case
DR_tokens <- tokens_tolower(DR_tokens)
DR_tokens <- tokens_select(DR_tokens, stopwords(), selection = "remove")
# Stemming
DR_tokens <- tokens_wordstem(DR_tokens, language = "english")
# biigrams
DR_tokens <- tokens_ngrams(DR_tokens, n = 2)
# Document Feature Matrix
DR_dfm <- dfm(DR_tokens, tolower = F)
# Matrix
DR_matrix <- as.matrix(DR_dfm)
library(doSNOW)
gc()
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
# TF
DR.df <- apply(DR_matrix, 1, term.frequency)
# IDf
DR.idf <-apply(DR_matrix, 2, inverse.doc.freq)
# TF_IDF
DR.tfidf <- apply(DR.df, 2, tf.idf,
idf = PDC.idf)
DR.tfidf <- t(DR.tfidf)
# Removing INcomplete cases
incomplete.cases <- which(!complete.cases(DR.tfidf))
DR.tfidf[incomplete.cases, ] <- rep(0.0, ncol(DR.tfidf))
incomplete.cases <- which(!complete.cases(DR.tfidf))
DR.tfidf[incomplete.cases, ] <- rep(0.0, ncol(DR.tfidf))
library(irlba)
gc()
registerDoSNOW(cl)
DR.irlba.1 <- irlba(t(DR.tfidf), nv = 10, maxit = 600)
DR.svd.1 <- data.frame(DR.irlba.1$v)
Data7 <- cbind(Data6, DR.svd.1)
Write.csv(Data7, "Data_For_Rating.csv")
write.csv(Data7, "Data_For_Rating.csv")
# Loading the Data
movies <- read.csv(file = "Data/tmdb_5000_movies.csv", header = T)
credits <- read.csv(file = "Data/tmdb_5000_credits.csv", header= T)
summary(movies)
summary(credits)
View(movies)
summary(movies$vote_average)
library(ggplot2)
ggplot(movies, aes(vote_average)) +
geom_bar(fill = "blue") +
xlab("Votes Average") +
theme_light()
# Focusing on the votes
range(movies$vote_count)
range(movies$vote_count)
ggplot(movies, aes(x = vote_count)) + geom_histogram(fill="red") +
theme_minimal()+
xlab("Number of Votes") +
ylab("Number of Movies")
# Loading the Data
movies <- read.csv(file = "Data/tmdb_5000_movies.csv", header = T)
credits <- read.csv(file = "Data/tmdb_5000_credits.csv", header= T)
# Data Analysis
summary(movies)
summary(credits)
head(movies)
summary(movies$vote_average)
dim(movies)
library(ggplot2)
ggplot(movies, aes(vote_average)) +
geom_bar(fill = "blue") +
xlab("Votes Average") +
theme_light()
# Focusing on the votes
range(movies$vote_count)
ggplot(movies, aes(x = vote_count)) + geom_histogram(fill="red") +
theme_minimal()+
xlab("Number of Votes") +
ylab("Number of Movies")
library(tidyverse)
library(jsonlite)
# Top Actors
cast <- credits %>%
filter(nchar(cast) > 2) %>%
mutate(js = lapply(cast, fromJSON)) %>%
unnest(js) %>%
select(-cast, -crew) %>%
rename(actor=name, movies_cast_id=cast_id, actor_id=id) %>%
mutate_if(is.character, factor)
cast1 <- cast %>% count(actor)
cast <- cast %>% filter(order %in% c(0, 1, 2)) %>% select(movie_id, title, order, actor)
cast$order[1] <- 0
for (i in 1:(nrow(cast) -1 )){
if(cast$movie_id[i+1] != cast$movie_id[i]){
cast$order[i + 1] <- 0
}else {cast$order[i+ 1] <- cast$order[i] + 1}
}
cast <- cast %>% filter(order %in% c(0, 1, 2)) %>%
spread(key = order, value = actor) %>%
rename(actor_1="0", actor_2="1", actor_3="2")
movies <- left_join(movies, cast %>% select(id = movie_id, actor_1, actor_2, actor_3), by = "id")
# Popular Directors
# Directors play a vital role in making a movie great
all_crew <- credits %>%
filter(nchar(crew)>2) %>%
mutate(
js  =  lapply(crew, fromJSON)
)  %>%
unnest(js)
director_wm<- all_crew %>%
filter(job=="Director") %>%
mutate(director=name) %>%
left_join(
movies,
by=c("movie_id" = "id")
)
# TOp Rated Directors
directors <- director_wm %>%
group_by(director) %>%
filter(vote_count>10) %>%
summarise(
n = n(),
weighted_mean =
weighted.mean(vote_average, vote_count)) %>%
filter(n>1) %>%
arrange(desc(weighted_mean))
credits$crew[10,]
credits$crew[,10]
View(director_wm)
# Visualizing
ggplot(directors) + stat_qq(aes(sample = weighted_mean))
ggplot(directors, aes(weighted_mean)) +
geom_histogram() +
theme_bw()
crew <- credits %>%
filter(nchar(crew)>2) %>%
mutate(js = lapply(crew, fromJSON)) %>%
unnest(js) %>%
select(-cast, -crew, -credit_id) %>%
rename(crew=name, crew_id=id) %>%
mutate_if(is.character, factor)
movies1Director <- crew %>% filter(job=="Director") %>% count(movie_id) %>% filter(n==1)
movies <- left_join(movies, crew %>% filter(job=="Director" & movie_id %in%
movies1Director$movie_id) %>%
select(id=movie_id, director=crew), by = "id")
View(movies)
genres <- movies %>% filter(nchar(genres) > 2) %>%
mutate(js = lapply(genres, fromJSON)) %>%
unnest(js, .name_repair = "unique") %>%
select(id, title, genres = name) %>%
mutate_if(is.character, factor)
genres %>% group_by(genres) %>% count() %>%
ggplot(aes(x=reorder(genres, n), y=n)) +
geom_col(fill="navyblue") + coord_flip() +
labs(x="", y="Number of movies")
gen <- genres
gen$order <- 0
gen$order[1] <- 1
for(i in 1:(nrow(gen)-1)) {
if(gen$id[i+1] !=gen$id[i]){
gen$order[i+1] <- 1
}else {gen$order[i+1] <- (gen$order[i]) + 1}
}
gen <- gen %>% filter(order < 4) %>%
spread(key=order, value=genres) %>%
rename(genre_1="1", genre_2="2", genre_3 = "3")
movies <- left_join(movies, gen %>% select(id, genre_1, genre_2, genre_3), by="id")
View(movies)
library(wordcloud)
# Keywords
keywords_counts <- keywords %>% count(keyword)
n_distinct(keywords$keyword)
keywords %>% count(keyword) %>% top_n(20, wt=n) %>%
ggplot(aes(x=reorder(keyword, n), y=n)) +
geom_col(fill="red") + coord_flip() +
labs(x="", y="Number of movies")
keywords <- movies %>%
filter(nchar(keywords)>2) %>%
mutate(
js = lapply(keywords, fromJSON)
) %>%
unnest(js, .name_repair = "unique") %>%
select(id, title, keyword=name) %>%
mutate_if(is.character, factor)
library(wordcloud)
# Keywords
keywords_counts <- keywords %>% count(keyword)
n_distinct(keywords$keyword)
keywords %>% count(keyword) %>% top_n(20, wt=n) %>%
ggplot(aes(x=reorder(keyword, n), y=n)) +
geom_col(fill="red") + coord_flip() +
labs(x="", y="Number of movies")
movies <- left_join(movies, keywords %>% select(keywords), by="id")
par(mfrow=c(1, 1),bg="white")
wordcloud(keywords_counts$keyword, keywords_counts$n, max.words = 100,
scale=c(2.0,.5), random.color = TRUE,
random.order=FALSE, rot.per=0, colors=brewer.pal(9,"Set1"))
production_companies <- movies %>% filter(nchar(production_companies) > 5) %>%
mutate(js = lapply(production_companies, fromJSON)) %>%
unnest(js, .name_repair = "unique") %>%
select(id, title, production_companies = name) %>%
mutate_if(is.character, factor)
production_companies <- movies %>% filter(nchar(production_companies) > 5) %>%
mutate(js = lapply(production_companies, fromJSON)) %>%
unnest(js, .name_repair = "unique") %>%
select(id, title, production_companies = name) %>%
mutate_if(is.character, factor)
PDC <- production_companies
PDC$order <- 0
PDC$order[1] <- 1
for(i in 1:(nrow(PDC)-1)) {
if(PDC$id[i+1] !=PDC$id[i]){
PDC$order[i+1] <- 1
}else {PDC$order[i+1] <- (PDC$order[i]) + 1}
}
PDC <- PDC %>% filter(order < 3) %>%
spread(key=order, value=production_companies) %>%
rename(PDC1="1", PDC2="2")
movies <- left_join(movies, PDC %>% select(id, PDC1, PDC2), by="id")
# movies <- movies[,-c(2, 3, 5, 10, 11, 15)]
write.csv(movies, "Movies_Processed.csv")
# Making the OverView of the movies a bit more usefull
Data <- read.csv(file = "Movies_Processed.csv", header = T)
View(Data)
library(quanteda)
# Separating overview from movies
overview <- Data$overview
overview <- as.data.frame(overview)
library(quanteda)
# Separating overview from movies
overview <- Data$overview
overview <- as.data.frame(overview)
View(overview)
# Tokenizing
OV_tokens <- tokens(overview$overview, what="word", remove_symbols = T,
remove_separators = T)
# Lower Case
OV_tokens <- tokens_tolower(OV_tokens)
OV_tokens <- tokens_select(OV_tokens, stopwords(), selection = "remove")
# Stemming
OV_tokens <- tokens_wordstem(OV_tokens, language = "english")
OV_tokens <- tokens(overview$overview, what="word", remove_symbols = T,
remove_separators = T)
# Lower Case
OV_tokens <- tokens_tolower(OV_tokens)
OV_tokens <- tokens_select(OV_tokens, stopwords(), selection = "remove")
# Stemming
OV_tokens <- tokens_wordstem(OV_tokens, language = "english")
# Unigrams
OV_tokens <- tokens_ngrams(OV_tokens, n = 1)
# Document Feature Matrix
OV_dfm <- dfm(OV_tokens, tolower = F)
# Matrix
OV_matrix <- as.matrix(OV_dfm)
View(OV_dfm)
View(OV_dfm)
View(overview)
term.frequency <- function(row){
row / sum(row)
}
# Take the row and divide by the total of the row
# Function for calculating inverse document frequency
# We want to look at the columns . so col
inverse.doc.freq <- function(col){
# Calculating documents in the columns
corpus.size <- length(col)
# Getting the number of rows where the column in not zero
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
# Function for calcularing TF-IDF
tf.idf <- function(x, idf){
x * idf
}
library(doSNOW)
gc()
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
# TF
OV.df <- apply(OV_matrix, 1, term.frequency)
# IDf
OV.idf <-apply(OV_matrix, 2, inverse.doc.freq)
# TF_IDF
OV.tfidf <- apply(OV.df, 2, tf.idf,
idf = OV.idf)
OV.tfidf <- t(OV.tfidf)
# Removing INcomplete cases
incomplete.cases <- which(!complete.cases(OV.tfidf))
OV.tfidf[incomplete.cases, ] <- rep(0.0, ncol(OV.tfidf))
incomplete.cases <- which(!complete.cases(OV.tfidf))
OV.tfidf[incomplete.cases, ] <- rep(0.0, ncol(OV.tfidf))
library(irlba)
gc()
registerDoSNOW(cl)
OV.irlba.1 <- irlba(t(OV.tfidf), nv = 10, maxit = 800)
OV.svd.1 <- data.frame(OV.irlba.1$v)
Data1 <- cbind(Data, OV.svd.1)
write.csv(Data1, "Svd_Processed_Data_set.csv")
write.csv(Data1, "Svd_Processed_Data_set.csv")
Data <- read.csv(file = "Svd_Processed_Data_set.csv", header = T)
View(Data)
num_features <- Data[,c(1, 9, 13, 14, 19, 20)]
View(num_features)
View(Data)
num_features <- Data[,c(3, 11, 15, 16, 21, 22)]
View(num_features)
library(corrplot)
corrplot(cor(num_features),
method = "number",
type = "upper"
)
cor(Data$vote_average, Data$vote_count)
cor(Data$revenue, Data$budget)
Data <- read.csv("Precossed_Data.csv", header = T)
Data <- read.csv("Precossed_Data.csv", header = T)
